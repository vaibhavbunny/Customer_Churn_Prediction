{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determing the number of hidden layers and neurons by hyperparameter tuning\n",
    "\n",
    "### This can be challenging and often requires experimentation. however there are some guidelines and \n",
    "### methods that can help you in making an informed decision.\n",
    "### Start Simple : Begin with a simple architecture and gradually increase complexity if needed \n",
    "### Grid Search/Random Search : Use GridSearch or RandomSearch to try out different architectures\n",
    "### Cross Validation : Use CV to evaluate the performance of difference of diffrent architectures \n",
    "### Heuristic and Rules of Thumb : \n",
    "### 1. Number of neurons in hidden layers should be between the size of the input layer and the size \n",
    "### of the output layer\n",
    "### 2. A common practice is to start with 1-2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikeras\n",
      "  Using cached scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting keras>=3.2.0 (from scikeras)\n",
      "  Using cached keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: scikit-learn>=1.4.2 in ./venv/lib/python3.11/site-packages (from scikeras) (1.6.1)\n",
      "Requirement already satisfied: absl-py in ./venv/lib/python3.11/site-packages (from keras>=3.2.0->scikeras) (2.2.2)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.11/site-packages (from keras>=3.2.0->scikeras) (1.26.4)\n",
      "Collecting rich (from keras>=3.2.0->scikeras)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.2.0->scikeras)\n",
      "  Using cached namex-0.0.9-py3-none-any.whl.metadata (322 bytes)\n",
      "Requirement already satisfied: h5py in ./venv/lib/python3.11/site-packages (from keras>=3.2.0->scikeras) (3.13.0)\n",
      "Collecting optree (from keras>=3.2.0->scikeras)\n",
      "  Using cached optree-0.15.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: ml-dtypes in ./venv/lib/python3.11/site-packages (from keras>=3.2.0->scikeras) (0.2.0)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.11/site-packages (from keras>=3.2.0->scikeras) (25.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./venv/lib/python3.11/site-packages (from scikit-learn>=1.4.2->scikeras) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./venv/lib/python3.11/site-packages (from scikit-learn>=1.4.2->scikeras) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib/python3.11/site-packages (from scikit-learn>=1.4.2->scikeras) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./venv/lib/python3.11/site-packages (from optree->keras>=3.2.0->scikeras) (4.13.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.2.0->scikeras)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.11/site-packages (from rich->keras>=3.2.0->scikeras) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
      "Using cached keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
      "Using cached namex-0.0.9-py3-none-any.whl (5.8 kB)\n",
      "Using cached optree-0.15.0-cp311-cp311-macosx_11_0_arm64.whl (338 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, optree, mdurl, markdown-it-py, rich, keras, scikeras\n",
      "\u001b[2K  Attempting uninstall: keras━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/7\u001b[0m [rich]\n",
      "\u001b[2K    Found existing installation: keras 2.15.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/7\u001b[0m [rich]\n",
      "\u001b[2K    Uninstalling keras-2.15.0:\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/7\u001b[0m [rich]\n",
      "\u001b[2K      Successfully uninstalled keras-2.15.00m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/7\u001b[0m [rich]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [scikeras]5/7\u001b[0m [keras]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-macos 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed keras-3.10.0 markdown-it-py-3.0.0 mdurl-0.1.2 namex-0.0.9 optree-0.15.0 rich-14.0.0 scikeras-0.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscikeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwrappers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KerasClassifier\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dense\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallbacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow.keras'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder,LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Churn_Modelling.csv')\n",
    "data = data.drop(columns=['RowNumber','CustomerId','Surname'],axis=1)\n",
    "scaler = StandardScaler()\n",
    "one_hot_encoder_geo = OneHotEncoder()\n",
    "lable_encoder_gender = LabelEncoder()\n",
    "data['Gender'] = lable_encoder_gender.fit_transform(data['Gender'])\n",
    "geo_encoded = one_hot_encoder_geo.fit_transform(data[['Geography']]).toarray()\n",
    "geo_encoded_df = pd.DataFrame(geo_encoded, columns=one_hot_encoder_geo.get_feature_names_out(['Geography']))\n",
    "\n",
    "data = pd.concat([data.drop('Geography',axis=1),geo_encoded_df],axis=1)\n",
    "x = data.drop('Exited',axis=1)\n",
    "y = data['Exited']\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "## dump all the models scalers as pickle files \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define a fucntion to create the model and try different parameters (KerasClassifier)\n",
    "\n",
    "def create_model(neurons=32,layers=1):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons,activation='relu',input_shape=(x_train.shape[1],)))\n",
    "    \n",
    "    for _ in range(layers-1):\n",
    "        model.add(Dense(neurons,activation='relu'))\n",
    "    \n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a kerasclassifier\n",
    "model = KerasClassifier(layers=1,neurons=32,build_fn=create_model,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the grid search parameters \n",
    "param_grid = {\n",
    "    'neurons' : [16,32,48,128],\n",
    "    'layers' : [1,2],\n",
    "    'epochs' : [50,100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = GridSearchCV(estimator=model,param_grid=param_grid,n_jobs=-1,cv=3)\n",
    "grid_result = grid.fit(x_train,y_train)\n",
    "\n",
    "## print the best parameters\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
